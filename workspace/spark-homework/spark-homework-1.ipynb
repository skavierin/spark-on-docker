{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e7ae24-6c37-432e-8b0b-cab5e48371f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "import requests as rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d77738-7a85-461a-a20e-441f10b4081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/08 12:36:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee9ec89-0b9c-46e9-b0a1-1dfe777338dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header=True).options(inferSchema='True').csv(\"/opt/workspace/spark-homework/raw-data\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b383af-007b-4bac-be0f-cd8d972fdde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter('Latitude is Not NULL').filter('Longitude is Not NULL')\n",
    "df = df[~df.col1.isnan() & !]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e448800f-c9f4-422d-8752-84bc8f96c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------+---------------+--------------------+----------+------------+\n",
      "|          Id|                Name|Country|           City|             Address|  Latitude|   Longitude|\n",
      "+------------+--------------------+-------+---------------+--------------------+----------+------------+\n",
      "|           2|Parkside Inn At I...|     US|Incline Village|1003 Tahoe Boulev...| 39.244493| -119.936437|\n",
      "|  8589934592|      Cadillac Motel|     US|     Brandywine|     16101 Crain Hwy|  38.66893|   -76.87629|\n",
      "| 17179869184|  Days Inn Brookings|     US|      Brookings|         2500 6th St|  44.31141|   -96.76286|\n",
      "| 17179869187|             Motel 6|     US|       Grayling|     6843 W M 72 Hwy| 44.657326|   -84.74439|\n",
      "| 25769803780|      Carleton Hotel|     US|       Carleton|       927 Monroe St|  42.05927|   -83.39095|\n",
      "| 42949672965|Comfort Inn Delan...|     US|         Deland|400 E Internation...| 29.054737|  -81.297208|\n",
      "| 51539607555|Magnuson Hotel Su...|     US|      Summerton|        18 Buff Blvd| 33.592181|  -80.356729|\n",
      "| 68719476738|     Southside Motel|     US|   Tappahannock|     910 S Church Ln| 37.923133|  -76.859002|\n",
      "| 77309411332|     Hotel Metropole|     US| Lithia Springs|Riva Degli Schiav...|  45.43393|    12.34551|\n",
      "| 85899345921| Country Inns Suites|     US|      Watertown|    3400 8th Ave S E|  44.89151|   -97.06111|\n",
      "| 85899345924|The Parkview Memphis|     US|        Memphis|     1914 Poplar Ave| 35.142477|  -89.996546|\n",
      "|103079215104|Wisconsin-aire Motel|     US|    Random Lake|N535 State Highwa...|  43.55906|   -87.94045|\n",
      "|111669149696|      Comfort Suites|     US|        Houston|1055 E Mcnee Road...| 29.681085|  -95.402996|\n",
      "|111669149699|Holiday Inn Expre...|     US|   Port Clinton|   50 N E Catawba Rd|  41.52469|   -82.85883|\n",
      "|120259084290|            Days Inn|     US|    Santa Maria|       839 E Main St|  34.95311|  -120.42399|\n",
      "|128849018882|Sheraton Offenbac...|     US|     Northbrook|Berliner Strasse 111| 50.106529|    8.760921|\n",
      "|128849018885|Marriott's Mounta...|     US|      Park City|     1305 Lowell Ave|  40.65143|  -111.50604|\n",
      "|137438953472|The Citizen Hotel...|     US|     Sacramento|            926 J St|38.5802767|-121.4938057|\n",
      "|137438953477|Holiday Inn Expre...|     US|       Columbus|    460 Waterbury Ct|  40.00747|  -82.865154|\n",
      "|146028888067|         Masters Inn|     US|       Savannah|     4200 Augusta Rd|  32.10299|    -81.1486|\n",
      "+------------+--------------------+-------+---------------+--------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0cb6011-186b-4af9-ab04-ec0473c1c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def forward_eval(lat, lon, name, city):\n",
    "    response = rq.get(f'https://api.opencagedata.com/geocode/v1/json?q={lat}+{lon}&key=bd6356b6fa7446e89545e28dc1bd8bab')\n",
    "    if response.status_code != 200:\n",
    "        return\n",
    "    \n",
    "    response_data = response.json()['results'][0]['components']\n",
    "\n",
    "    building_true = response_data['building']\n",
    "    city_true = response_data['city']\n",
    "\n",
    "    if name in building_true or building_true in name and city_true in city or city in city_true:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "463d97ca-e544-4d3e-a007-aba2eb326975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('geo_filter', forward_eval(df.Latitude, df.Longitude, df.Name, df.City))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efe62ee7-3e1d-4a86-b4bf-a7b580153d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/08 12:38:38 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 5) (172.18.0.5 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'requests'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/02/08 12:38:38 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'requests'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 603, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 449, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 251, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/usr/bin/spark-3.2.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'requests'\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f178082-1da9-4a37-ac53-74314c0e5dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
